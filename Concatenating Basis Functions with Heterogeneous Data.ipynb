{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Feature Union with Heterogeneous Data Sources](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial basis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial basis function is provided by `scikit-learn` in the [sklearn.preprocessing](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6],\n",
       "       [7, 8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(1, 9).reshape(4, 2)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  2,  1,  2,  4],\n",
       "       [ 1,  3,  4,  9, 12, 16],\n",
       "       [ 1,  5,  6, 25, 30, 36],\n",
       "       [ 1,  7,  8, 49, 56, 64]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures(degree=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this is pretty much the extent of what `scikit-learn` provides in the way of basis functions. Here we define some standard basis functions, while adhering to the `scikit-learn` interface. This will be important when we try to incorporate our basis functions in pipelines and feature unions later on. While this is not strictly required, it will certainly make life easier for us down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Basis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RadialFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, mu=0, s=1):\n",
    "        self.mu = mu\n",
    "        self.s = s\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # this basis function stateless\n",
    "        # need only return self\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return np.exp(-cdist(X, self.mu, 'sqeuclidean')/(2*self.s**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoidal Basis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidalFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, mu=0, s=1):\n",
    "        self.mu = mu\n",
    "        self.s = s\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # this basis function stateless\n",
    "        # need only return self\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return expit(cdist(X, self.mu)/self.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  0.2],\n",
       "       [ 0.3,  0.4],\n",
       "       [ 0.5,  0.6],\n",
       "       [ 0.7,  0.8],\n",
       "       [ 0.9,  1. ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = np.linspace(0.1, 1, 10).reshape(5, 2)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13,  0.22,  0.33,  0.47,  0.6 ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RadialFeatures(mu=mu).fit_transform(X).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88,  0.85,  0.82,  0.78,  0.73],\n",
       "       [ 0.99,  0.99,  0.99,  0.98,  0.97],\n",
       "       [ 1.  ,  1.  ,  1.  ,  1.  ,  1.  ],\n",
       "       [ 1.  ,  1.  ,  1.  ,  1.  ,  1.  ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SigmoidalFeatures(mu=mu).fit_transform(X).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a few basis functions at our disposal, let's try to apply different basis functions to different features of a dataset. We use the diabetes dataset, a real-world dataset with 442 instances and 10 features. We first work through each step manually, and show how the steps can be combined using `scikit-learn`'s feature unions and pipelines to form a single model that will perform all the necessary steps in one fell swoop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print every other feature for just the first few instances, just to get an idea of what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.06169621, -0.0442235 , -0.04340085,  0.01990842],\n",
       "       [-0.00188202, -0.05147406, -0.00844872,  0.07441156, -0.06832974],\n",
       "       [ 0.08529891,  0.04445121, -0.04559945, -0.03235593,  0.00286377],\n",
       "       [-0.08906294, -0.01159501,  0.01219057, -0.03603757,  0.02269202],\n",
       "       [ 0.00538306, -0.03638469,  0.00393485,  0.00814208, -0.03199144]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "X[:5, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 151.,   75.,  141.,  206.,  135.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume for some reason we are interested in training a model using, say, features 2 and 5 with a *polynomial basis*, and features 6, 8 and 9 with a *radial basis*. We first slice up our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = X[:, np.array([2, 5])]\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06169621, -0.03482076],\n",
       "       [-0.05147406, -0.01916334],\n",
       "       [ 0.04445121, -0.03419447],\n",
       "       [-0.01159501,  0.02499059],\n",
       "       [-0.03638469,  0.01559614]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "X1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = X[:, np.array([6, 8, 9])]\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04340085,  0.01990842, -0.01764613],\n",
       "       [ 0.07441156, -0.06832974, -0.09220405],\n",
       "       [-0.03235593,  0.00286377, -0.02593034],\n",
       "       [-0.03603757,  0.02269202, -0.00936191],\n",
       "       [ 0.00814208, -0.03199144, -0.04664087]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "X2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the respective basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_poly = PolynomialFeatures().fit_transform(X1)\n",
    "X1_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.06, -0.03,  0.  , -0.  ,  0.  ],\n",
       "       [ 1.  , -0.05, -0.02,  0.  ,  0.  ,  0.  ],\n",
       "       [ 1.  ,  0.04, -0.03,  0.  , -0.  ,  0.  ],\n",
       "       [ 1.  , -0.01,  0.02,  0.  , -0.  ,  0.  ],\n",
       "       [ 1.  , -0.04,  0.02,  0.  , -0.  ,  0.  ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "X1_poly[:5].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0.2,  0.4],\n",
       "       [ 0.6,  0.8,  1. ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = np.linspace(0, 1, 6).reshape(2, 3)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_radial = RadialFeatures(mu).fit_transform(X2)\n",
    "X2_radial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9 ,  0.36],\n",
       "       [ 0.85,  0.33],\n",
       "       [ 0.9 ,  0.35],\n",
       "       [ 0.9 ,  0.36],\n",
       "       [ 0.88,  0.34]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "X2_radial[:5].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to concatenate these augmented datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_concat = np.hstack((X1_poly, X2_radial))\n",
    "X_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  , -0.03, -0.  ,  0.9 ],\n",
       "       [ 1.  , -0.02,  0.  ,  0.85],\n",
       "       [ 1.  , -0.03, -0.  ,  0.9 ],\n",
       "       [ 1.  ,  0.02, -0.  ,  0.9 ],\n",
       "       [ 1.  ,  0.02, -0.  ,  0.88]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "X_concat[:5, ::2].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train a regressor with this augmented dataset. For this example, we'll simply use a linear regression model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_concat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41693404706575754"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_concat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(To no one's surprise, our model performs quite poorly, since zero effort was made to identify and incorporate the most informative features or appropriate basis functions. Rather, they were chosen solely to maximize clarity of exposition.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap\n",
    "\n",
    "So let's recap what we've done.\n",
    "\n",
    "1. We started out with a dataset with 442 samples and 10 features, represented by **442x10 matrix `X`**\n",
    "2. For one reason or another, we wanted to use different basis functions for different subsets of features. Apparently, we wanted features 2 and 5 for one basis function and features 6, 8 and 9 for another. Therefore, we \n",
    "   1. sliced the matrix `X` to obtain **442 by 2 matrix `X1`** and \n",
    "   2. sliced the matrix `X` to obtain **442 by 3 matrix `X2`**.\n",
    "3. We\n",
    "   1. applied a polynomial basis function of degree 2 to `X1` with 2 features and 442 samples. This returns a dataset `X1_poly` with $\\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} = 6$ features and 442 samples. (**NB:** In general, the number of output features for a polynomial basis function of degree $d$ on $n$ features is the number of multisets of cardinality $d$, with elements taken from a finite set of cardinality $n+1$, which is given by the multiset coefficient $\\begin{pmatrix} \\begin{pmatrix} n + 1 \\\\ d \\end{pmatrix} \\end{pmatrix} = \\begin{pmatrix} n + d \\\\ d \\end{pmatrix}$.) So from 442 by 2 matrix `X1` we obtain **442 by 6 matrix `X1_poly`**\n",
    "   2. applied a radial basis function with 2 mean vectors $\\mu_1 = \\begin{pmatrix} 0 & 0.2 & 0.4 \\end{pmatrix}^T$ and $\\mu_2 = \\begin{pmatrix} 0.6 & 0.8 & 1.0 \\end{pmatrix}^T$, which is represented by the 2 by 3 matrix `mu`. From the 442 by 3 matrix `X2`, we obtain **442 by 2 matrix `X2_radial`**\n",
    "4. Next, we horizontally concatenated 442 by 6 matrix `X1_poly` with 442 by 2 matrix `X2_radial` to obtain the final **442 by 8 matrix `X_concat`**\n",
    "5. Finally, we fitted a linear model on `X_concat`.\n",
    "\n",
    "So this is how we went from a 442x**10** matrix `X` to the 442x**8** matrix `X_concat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Pipeline and Feature Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a transformer that slices up the input data. Note instead of working with (tuples of) slice objects, it is usually more convenient to use the Numpy function `np.index_exp`. We explain later why this is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ArraySlicer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, index_exp):\n",
    "        self.index_exp = index_exp\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.index_exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \\\n",
    "make_pipeline(\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            ArraySlicer(np.index_exp[:, np.array([2, 5])]),\n",
    "            PolynomialFeatures()\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            ArraySlicer(np.index_exp[:, np.array([6, 8, 9])]),\n",
    "            RadialFeatures(mu)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('arrayslicer', ArraySlicer(index_exp=(slice(None, None, None), array([2, 5])))), ('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False))])), ('pipeline-2', ...rray([[ 0. ,  0.2,  0.4],\n",
       "       [ 0.6,  0.8,  1. ]]), s=1))]))],\n",
       "       transformer_weights=None))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  , -0.03, -0.  ,  0.9 ],\n",
       "       [ 1.  , -0.02,  0.  ,  0.85],\n",
       "       [ 1.  , -0.03, -0.  ,  0.9 ],\n",
       "       [ 1.  ,  0.02, -0.  ,  0.9 ],\n",
       "       [ 1.  ,  0.02, -0.  ,  0.88]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "model.transform(X)[:5, ::2].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effectively composes each of the steps we had to manually perform and amalgamated it into a single transformer. We can even append a regressor at the end to make it a complete estimator/predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = \\\n",
    "make_pipeline(\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            ArraySlicer(np.index_exp[:, np.array([2, 5])]),\n",
    "            PolynomialFeatures()\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            ArraySlicer(np.index_exp[:, np.array([6, 8, 9])]),\n",
    "            RadialFeatures(mu)\n",
    "        )\n",
    "    ),\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('arrayslicer', ArraySlicer(index_exp=(slice(None, None, None), array([2, 5])))), ('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False))])), ('pipeline-2', ... ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41693404706575754"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking it Down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing to note is that everything in `scikit-learn` is either a transformer or a predictor, and are almost always an estimator. An estimator is simply a class that implements the `fit` method, while a transfromer and predictor implements a, well, `transform` and `predict` method respectively. From this simple interface, we get a surprising hight amount of functionality and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline behaves as a transformer or a predictor depending on what the last step of the pipleline is. If the last step is a transformer, the entire pipeline is a transformer and one can call `fit`, `transform` or `fit_transform` like an ordinary transformer. The same is true if the last step is a predictor. Essentially, all it does is chain the `fit_transform` calls of every transformer in the pipeline. If we think of ordinary transformers like functions, pipelines can be thought of as a higher-order function that simply composes an arbitary number of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = \\\n",
    "make_pipeline(\n",
    "    PolynomialFeatures(), # transformer\n",
    "    LinearRegression() # predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59243942644119341"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A union is a transformer that is initialized with an arbitrary number of transformers. When `fit_transform` is called on a dataset, it simply calls `fit_transform` of the transformers it was given and horizontally concatenates its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu_ = np.linspace(0, 10, 30).reshape(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = \\\n",
    "make_union(\n",
    "    PolynomialFeatures(),\n",
    "    RadialFeatures(mu_)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run this on the original 442x10 dataset, we expect to get a dataset with the same number of samples and $\\begin{pmatrix} 12 \\\\ 2 \\end{pmatrix} + 3 = 66 + 3 = 69$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 69)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above union applies the basis functions on the entire dataset, but we're interested in applying different basis functions to different features. To do this, we can simply define a rather frivolous transformer that simply slices the input data, and that's exactly what `ArraySlicer` was for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = \\\n",
    "make_pipeline(\n",
    "    ArraySlicer(np.index_exp[:, np.array([2, 5])]),\n",
    "    PolynomialFeatures()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('arrayslicer', ArraySlicer(index_exp=(slice(None, None, None), array([2, 5])))), ('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 6)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.06, -0.03,  0.  , -0.  ,  0.  ],\n",
       "       [ 1.  , -0.05, -0.02,  0.  ,  0.  ,  0.  ],\n",
       "       [ 1.  ,  0.04, -0.03,  0.  , -0.  ,  0.  ],\n",
       "       [ 1.  , -0.01,  0.02,  0.  , -0.  ,  0.  ],\n",
       "       [ 1.  , -0.04,  0.02,  0.  , -0.  ,  0.  ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "model.transform(X)[:5].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can combine this all together to form our mega-transformer which we showed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = \\\n",
    "make_pipeline(\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            ArraySlicer(np.index_exp[:, np.array([2, 5])]),\n",
    "            PolynomialFeatures()\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            ArraySlicer(np.index_exp[:, np.array([6, 8, 9])]),\n",
    "            RadialFeatures(mu)\n",
    "        )\n",
    "    ),\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a predictor which takes some input, slices up the respective features, churns it through a basis function and finally trains a linear regressor on it, all in one go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('featureunion', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('pipeline-1', Pipeline(steps=[('arrayslicer', ArraySlicer(index_exp=(slice(None, None, None), array([2, 5])))), ('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False))])), ('pipeline-2', ... ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41693404706575754"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating Variable and Keyword arguments in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
